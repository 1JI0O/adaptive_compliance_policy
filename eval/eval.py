"""
Evaluation.
"""
import sys
import pathlib
import os
import hydra
import torch
from omegaconf import OmegaConf
from torch.utils.data import DataLoader
import copy
import random
import wandb
import pickle
import tqdm
import numpy as np
import shutil
from collections import deque
from datetime import datetime
import cv2

import yaml
from easydict import EasyDict as edict

# 1. ç®—å‡ºæ ¹ç›®å½•
ROOT_DIR = str(pathlib.Path(__file__).parent.parent.absolute())
# 2. ç®—å‡º PyriteML æ‰€åœ¨çš„ç›®å½•
PYRITE_ML_DIR = os.path.join(ROOT_DIR, 'PyriteML')

# å°†è¿™ä¸¤ä¸ªéƒ½åŠ å…¥ç¯å¢ƒå˜é‡
sys.path.append(ROOT_DIR)
sys.path.append(PYRITE_ML_DIR)

os.chdir(ROOT_DIR)

from PyriteML.diffusion_policy.workspace.base_workspace import BaseWorkspace
from PyriteML.diffusion_policy.policy.diffusion_unet_timm_mod1_policy import (
    DiffusionUnetTimmMod1Policy,
)

import PyriteUtility.spatial_math.spatial_utilities as su

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



# from diffusion_policy.policy.diffusion_unet_image_policy import DiffusionUnetImagePolicy
from PyriteML.diffusion_policy.dataset.base_dataset import BaseImageDataset, BaseDataset

# from diffusion_policy.env_runner.base_image_runner import BaseImageRunner
from diffusion_policy.common.checkpoint_util import TopKCheckpointManager
from diffusion_policy.common.json_logger import JsonLogger
from diffusion_policy.common.pytorch_util import dict_apply, optimizer_to
from diffusion_policy.model.diffusion.ema_model import EMAModel
from diffusion_policy.model.common.lr_scheduler import get_scheduler
from accelerate import Accelerator

from scipy.spatial.transform import Rotation as R

from eval_agent import SingleArmAgent

# # å›¾åƒè§‚æµ‹ï¼šçœ‹æœ€è¿‘ 2 å¸§ï¼Œæ­¥é•¿ä¸º 1 (é—´éš”çº¦ 50ms)
# sparse_obs_rgb_down_sample_steps = 1
# sparse_obs_rgb_horizon = 2

# # ä½ç»´çŠ¶æ€ï¼ˆPoseï¼‰ï¼šçœ‹æœ€è¿‘ 3 å¸§
# sparse_obs_low_dim_down_sample_steps = 1
# sparse_obs_low_dim_horizon = 3

# # åŠ›çŸ©ï¼ˆWrenchï¼‰ï¼šåŠ›çŸ©é€šå¸¸éœ€è¦æ›´é•¿çš„å†å²ä¿¡æ¯ã€‚
# sparse_obs_wrench_down_sample_steps = 1
# sparse_obs_wrench_horizon = 32
# # åŠ¨ä½œé¢„æµ‹ï¼ˆActionï¼‰ï¼šé¢„æµ‹æœªæ¥ 16 å¸§ï¼ˆçº¦ 0.8s çš„åŠ¨ä½œè½¨è¿¹ï¼‰
# sparse_action_down_sample_steps = 1
# sparse_action_horizon = 16

# RGBï¼ˆ15 Hz ç›¸æœºï¼‰
sparse_obs_rgb_down_sample_steps : 1
sparse_obs_rgb_horizon : 2

# Poseï¼ˆ1000 Hzï¼Œä½†åªéœ€è¦çŸ­æœŸï¼‰
sparse_obs_low_dim_down_sample_steps : 1
sparse_obs_low_dim_horizon : 3

# Wrenchï¼ˆ1000 Hzï¼Œéœ€è¦é•¿æœŸå†å² + 1D Conv å¤„ç†ï¼‰
sparse_obs_wrench_down_sample_steps : 5   # ğŸ”¥ å…³é”®ï¼šæ‰©å¤§æ—¶é—´æ„Ÿå—é‡
sparse_obs_wrench_horizon : 32            # ğŸ”¥ å…³é”®ï¼šè¶³å¤Ÿçš„æ ·æœ¬ç»™ 1D Conv

# Action
sparse_action_down_sample_steps : 1
sparse_action_horizon : 16

# ä»¥ä¸Šè¿™äº›å‚æ•°å¯ä»¥ä»yamlé‡Œé¢è¯»å–ï¼Œå…ˆå®ç°ä¸»å¹²é€»è¾‘

yaml_path = "/home/flexiv/data/acp/.hydra/config.yaml"
ckpt_path = "/home/flexiv/data/acp/latest.ckpt"
max_steps = 3000
eval_config_path = "/home/flexiv/git/adaptive_compliance_policy/eval/eval_config.yaml"
normalizer_path = "/home/flexiv/data/acp/sparse_normalizer.pkl"

# color_path = "/data/haoxiang/acp/flip_v3/scene_0001/cam_104122060902/color/1768287143577.png"

# yaml_path = "/data/haoxiang/logs/acp_logs/2026.01.20_04.50.05_flip_new_v3_conv_230/.hydra/config.yaml"
# ckpt_path = "/data/haoxiang/logs/acp_logs/2026.01.20_04.50.05_flip_new_v3_conv_230/checkpoints/latest.ckpt"
# max_steps = 3000
# # eval_config_path = "/home/flexiv/git/adaptive_compliance_policy/eval/eval_config.yaml"
# normalizer_path = "/data/haoxiang/logs/acp_logs/2026.01.20_04.50.05_flip_new_v3_conv_230/sparse_normalizer.pkl"


n_action_steps = 8  

# === åˆå§‹åŒ– Buffer ===
# ä½¿ç”¨ deque æ¥è‡ªåŠ¨ç»´æŠ¤æ»‘åŠ¨çª—å£
buffer_rgb_0 = deque(maxlen=sparse_obs_rgb_horizon)  # ç›¸æœº 0
buffer_rgb_1 = deque(maxlen=sparse_obs_rgb_horizon)  # ç›¸æœº 1
buffer_pos = deque(maxlen=sparse_obs_low_dim_horizon)
buffer_rot = deque(maxlen=sparse_obs_low_dim_horizon)
buffer_wrench = deque(maxlen=sparse_obs_wrench_horizon)

action_queue = deque(maxlen=100)

# export PYRITE_CHECKPOINT_FOLDERS=/home/flexiv/data/acp

def reset_buffers():
    buffer_rgb_0.clear()
    buffer_rgb_1.clear()  # ğŸ”¥ æ–°å¢
    buffer_pos.clear()
    buffer_rot.clear()
    buffer_wrench.clear()

def load_test_obs(color_path):
    # 1. åŠ è½½å½©è‰²å›¾å¹¶è½¬ä¸º RGB (OpenCV é»˜è®¤è¯»å…¥æ˜¯ BGR)
    color_image = cv2.imread(color_path)
    if color_image is None:
        raise ValueError(f"æ— æ³•åŠ è½½å›¾ç‰‡: {color_path}")
    color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB).astype(np.uint8)

    return color_image

OmegaConf.register_new_resolver(
    "now", 
    lambda pattern: datetime.now().strftime(pattern), 
    replace=True
)

def evaluate():

    # cfg = OmegaConf.load(yaml_path)
    # policy = hydra.utils.instantiate(cfg.policy)

    # with open(eval_config_path, "r") as f:
    #     eval_config = edict(yaml.load(f, Loader = yaml.FullLoader))
    #     # è¿™ä¸ªä¸»è¦æ˜¯agentç›¸å…³çš„config

    # # load checkpoint
    # ckpt = torch.load(ckpt_path, map_location='cpu')
    # if "state_dicts" in ckpt:
    #     policy.load_state_dict(ckpt["state_dicts"]["policy"], strict=False)
    # else:
    #     print("abnormal ckpt load!")
    #     policy.load_state_dict(ckpt, strict=False)

	# è¿™ä¸ª config_path éœ€è¦æŒ‡å®šä¸ºconfig.yamlçš„ä½ç½®

    # 2. åŠ è½½å¹¶è§£æé…ç½®
    cfg = OmegaConf.load(yaml_path)
    OmegaConf.resolve(cfg) # è¿™ä¸€æ­¥å¿…ä¸å¯å°‘ï¼Œè§£ææ‰€æœ‰ ${task.name} ç­‰å˜é‡

    # 3. åˆ©ç”¨ Hydra å®ä¾‹åŒ–æ•´ä¸ª Policy ç½‘ç»œç»“æ„
    # å®ƒä¼šè‡ªåŠ¨åˆ›å»º TimmObsEncoderWithForce, DDIMScheduler, ä»¥åŠ DiffusionUnet
    policy = hydra.utils.instantiate(cfg.policy)

        # --- å…³é”®æ­¥éª¤ï¼šåŠ è½½å¹¶ä¼ å…¥ Normalizer ---
    if os.path.exists(normalizer_path):
        with open(normalizer_path, 'rb') as f:
            normalizer_data = pickle.load(f)
        
        policy.set_normalizer(normalizer_data)

    
    # 4. åŠ è½½æƒé‡
    payload = torch.load(ckpt_path, map_location=device)
    policy.load_state_dict(payload['state_dicts']['ema_model'])
    policy = policy.to(device)
    

    # set evaluation
    policy.eval()

    with open(eval_config_path, "r") as f:
        eval_config = edict(yaml.load(f, Loader = yaml.FullLoader))
        # è¿™ä¸ªä¸»è¦æ˜¯agentç›¸å…³çš„config

    # # initialize agent
    Agent = SingleArmAgent
    agent = Agent(**eval_config.deploy.agent)

    # evaluation rollout
    print("Ready for rollout. Press Enter to continue...")
    input()
    
    with torch.inference_mode():
        for t in range(max_steps):

            print(f"Step {t} ---------------------")
           

            rgb_raw_0, rgb_raw_1 = agent.get_global_observation() # (H_raw, W_raw, 3), uint8
            # è¿™é‡Œéœ€è¦ä¿®æ”¹agentå®ç°


            # ğŸ”¥ åˆ†åˆ«å¤„ç†ä¸¤ä¸ªç›¸æœºçš„å›¾åƒ
            # ç›¸æœº 0
            rgb_resized_0 = cv2.resize(rgb_raw_0, (224, 224), interpolation=cv2.INTER_AREA)
            rgb_0 = rgb_resized_0.transpose(2, 0, 1)  # (3, 224, 224)
            
            # ç›¸æœº 1
            rgb_resized_1 = cv2.resize(rgb_raw_1, (224, 224), interpolation=cv2.INTER_AREA)
            rgb_1 = rgb_resized_1.transpose(2, 0, 1)  # (3, 224, 224)

            proprio = agent.get_proprio() # [x, y, z, rot6d, gripper]
            # get_proprio å·²ç» xyz_rot_transform åˆ°å…­å…ƒæ•°äº†ï¼Œä¸ç”¨å†æ¬¡è½¬æ¢
            end_pos = proprio[:3]
            end_rot6d = proprio[3:9]

            wrench = agent.get_wrench()
            
            # è€ƒè™‘steps
            if t % sparse_obs_rgb_down_sample_steps == 0:
                buffer_rgb_0.append(rgb_0)
                buffer_rgb_1.append(rgb_1)

            if t % sparse_obs_low_dim_down_sample_steps == 0:
                buffer_pos.append(end_pos)
                buffer_rot.append(end_rot6d)
            if t % sparse_obs_wrench_down_sample_steps == 0:
                buffer_wrench.append(wrench)

            # Padding: å¦‚æœæ˜¯ç¬¬ä¸€å¸§ï¼ŒæŠŠ Buffer å¡«æ»¡ï¼Œé˜²æ­¢é•¿åº¦ä¸å¤ŸæŠ¥é”™
            if t == 0:
                while len(buffer_rgb_0) < sparse_obs_rgb_horizon: buffer_rgb_0.append(rgb_0)
                while len(buffer_rgb_1) < sparse_obs_rgb_horizon: buffer_rgb_1.append(rgb_1)
                while len(buffer_pos) < sparse_obs_low_dim_horizon: buffer_pos.append(end_pos)
                while len(buffer_rot) < sparse_obs_low_dim_horizon: buffer_rot.append(end_rot6d)
                while len(buffer_wrench) < sparse_obs_wrench_horizon: buffer_wrench.append(wrench)

            if len(buffer_pos) < sparse_obs_low_dim_horizon:
                print(f"Step {t}: Buffer not ready, skipping prediction")
                continue  # è·³è¿‡ï¼Œç­‰å¾… buffer å¡«æ»¡

            # åŠ¨ä½œé˜Ÿåˆ—ä¸ºç©ºï¼Œä¸Šä¸€æ‰¹åŠ¨ä½œå…¨éƒ¨æ‰§è¡Œå®Œåå†é¢„æµ‹

            if len(action_queue) == 0:

                # ========================================
                # ğŸ”¥ è§‚æµ‹ç›¸å¯¹åŒ–ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                # ========================================
                base_pos = buffer_pos[-1]
                base_rot6d = buffer_rot[-1]
                base_pose9 = np.concatenate([base_pos, base_rot6d])
                base_SE3 = su.pose9_to_SE3(base_pose9)
                
                buffer_pos_relative = []
                buffer_rot_relative = []
                for pos, rot6d in zip(buffer_pos, buffer_rot):
                    pose9 = np.concatenate([pos, rot6d])
                    SE3 = su.pose9_to_SE3(pose9)
                    SE3_relative = su.SE3_inv(base_SE3) @ SE3
                    pose9_relative = su.SE3_to_pose9(SE3_relative)
                    buffer_pos_relative.append(pose9_relative[:3])
                    buffer_rot_relative.append(pose9_relative[3:9])
                
                # æ„å»º batchï¼ˆä½¿ç”¨ç›¸å¯¹åŒ–çš„è§‚æµ‹ï¼‰
                obs_batch = {
                    "sparse": {
                        "rgb_0": torch.from_numpy(np.stack(list(buffer_rgb_0))).unsqueeze(0).float().to(device),
                        "rgb_1": torch.from_numpy(np.stack(list(buffer_rgb_1))).unsqueeze(0).float().to(device),
                        "robot0_eef_pos": torch.from_numpy(np.stack(buffer_pos_relative)).unsqueeze(0).float().to(device),
                        "robot0_eef_rot_axis_angle": torch.from_numpy(np.stack(buffer_rot_relative)).unsqueeze(0).float().to(device),
                        "robot0_eef_wrench": torch.from_numpy(np.stack(list(buffer_wrench))).unsqueeze(0).float().to(device)
                    }
                }

                # result,stiffness_unnorm,raw_pred = policy.predict_action(obs_batch)
                # print("Predicted raw action:", raw_pred)
                # time ç»´é•¿åº¦æ˜¯ sparse_action_horizon

                result = policy.predict_action(obs_batch)

                all_pred_actions = result['sparse'].squeeze(0).cpu().numpy()
                # 9 for reference pose, 9 for virtual target, 1 for stiffness

                all_pred_stiff_raw = stiffness_unnorm.squeeze(0).cpu().numpy()

                # ========================================
                # ğŸ”¥ æ–°å¢ï¼šå°†ç›¸å¯¹åŠ¨ä½œè½¬æ¢ä¸ºç»å¯¹åŠ¨ä½œ
                # ========================================
                # ç›¸å¯¹åˆå§‹ä½ç½®

                current_SE3 = base_SE3

                # éå†æ¯ä¸€æ­¥åŠ¨ä½œï¼Œè½¬æ¢ä¸ºç»å¯¹åæ ‡
                all_pred_actions_absolute = []
                for i, relative_action in enumerate(all_pred_actions):
                    # æå–ç›¸å¯¹ä½å§¿å’Œåˆšåº¦
                    ref_pose9_rel = relative_action[0:9]
                    vt_pose9_rel = relative_action[9:18]
                    stiffness_val = relative_action[18]

                    # è½¬æ¢ä¸º SE3 çŸ©é˜µ
                    ref_SE3_rel = su.pose9_to_SE3(ref_pose9_rel)
                    vt_SE3_rel = su.pose9_to_SE3(vt_pose9_rel)

                    # ğŸ”¥ å…³é”®æ“ä½œï¼šç›¸å¯¹ â†’ ç»å¯¹
                    ref_SE3_abs = current_SE3 @ ref_SE3_rel
                    vt_SE3_abs = current_SE3 @ vt_SE3_rel

                    # è½¬å› pose9 æ ¼å¼
                    ref_pose9_abs = su.SE3_to_pose9(ref_SE3_abs)
                    vt_pose9_abs = su.SE3_to_pose9(vt_SE3_abs)

                    # æ‹¼æ¥æˆå®Œæ•´åŠ¨ä½œ
                    absolute_action = np.concatenate([
                        ref_pose9_abs,      # å‚è€ƒä½å§¿ï¼ˆç»å¯¹ï¼‰
                        vt_pose9_abs,       # è™šæ‹Ÿç›®æ ‡ï¼ˆç»å¯¹ï¼‰
                        [stiffness_val]     # åˆšåº¦ä¿æŒä¸å˜
                    ])
                    all_pred_actions_absolute.append(absolute_action)

                all_pred_actions_absolute = np.array(all_pred_actions_absolute)

                print(all_pred_actions)
                print("=" * 60)
                print(all_pred_actions_absolute)

                # åªæ‰§è¡Œå‰ n_action_steps
                # steps_to_execute = all_pred_actions[:n_action_steps]
                steps_to_execute = all_pred_actions_absolute[:n_action_steps]

                # å°†åŠ¨ä½œæ¨å…¥é˜Ÿåˆ—
                for act in steps_to_execute:
                    action_queue.append(act)

            
            # æ‰§è¡ŒåŠ¨ä½œ

            # ä»é˜Ÿåˆ—ä¸­å‡ºé˜Ÿä¸€ä¸ªåŠ¨ä½œæ‰§è¡Œ
            raw_action = action_queue.popleft() 

            # print("Raw action to execute:", raw_action)

            # Slice 1: Reference Pose 
            ref_pos = raw_action[0:3]
            ref_rot_6d = raw_action[3:9]

            # Slice 2: Virtual Target
            vt_pos = raw_action[9:12]
            vt_rot_6d = raw_action[12:18]

            # get step_action
            step_action = raw_action[9:18]

            # Slice 3: Stiffness
            stiffness_val = raw_action[18]

            # process stiffness

            # 2. å‡†å¤‡åˆšåº¦å‚æ•°
            K_MAX = 10000  # ç¡¬
            K_MIN = 200.0   # è½¯
            K_ROT = 500   # æ—‹è½¬åˆšåº¦

            # è®¡ç®— k_low (æ¨¡å‹è¾“å‡º 0~1 æ˜ å°„åˆ° K_MIN~K_MAX)
            # k_low = K_MIN + stiffness_val * (K_MAX - K_MIN)
            # k_low = K_MIN + stiffness_unnorm * (K_MAX - K_MIN)

            # print("stiffness raw:", stiffness_unnorm)
            

            # 3. --- æ ¸å¿ƒï¼šè®¡ç®— Force Frame ---
            # å‘é‡æ–¹å‘ï¼šä» Ref æŒ‡å‘ VT
            diff = np.array(vt_pos) - np.array(ref_pos)
            dist = np.linalg.norm(diff)

            if dist < 1e-6:
                # å¦‚æœé‡åˆï¼Œæ²¡æœ‰ç‰¹å®šæ–¹å‘ï¼Œå°±ç”¨é»˜è®¤çš„ä¸–ç•Œåæ ‡ç³»ï¼ˆæ— æ—‹è½¬ï¼‰
                # åˆšåº¦å…¨å‘è®¾ä¸ºæœ€ç¡¬
                rotation_matrix = np.eye(3)
                k_x = K_MAX 
            else:
                # --- æ„å»ºæ—‹è½¬çŸ©é˜µ ---
                # 1. Xè½´ï¼šä¸»æ–¹å‘
                x_axis = diff / dist
                
                # 2. Yè½´ï¼šæ‰¾ä¸€ä¸ªè¾…åŠ©å‘é‡åšå‰ä¹˜
                temp_vec = np.array([0, 0, 1.0])
                if np.abs(np.dot(x_axis, temp_vec)) > 0.99: # é˜²æ­¢å…±çº¿
                    temp_vec = np.array([0, 1.0, 0])
                
                y_axis = np.cross(x_axis, temp_vec)
                y_axis /= np.linalg.norm(y_axis)
                
                # 3. Zè½´
                z_axis = np.cross(x_axis, y_axis)
                
                # 4. ç»„åˆæˆçŸ©é˜µ (åˆ—å‘é‡)
                rotation_matrix = np.column_stack((x_axis, y_axis, z_axis))
                
                # åˆšåº¦ï¼šåªæœ‰ X è½´æ˜¯è½¯çš„
                # k_x = k_low
                k_x = stiffness_val
                # ç”±äº policy è¾“å‡º pred_action å·²ç» unnorm äº†ï¼Œè¿™é‡Œç›´æ¥ç”¨

            if k_x > K_MAX:
                k_x = K_MAX

            force_frame = np.eye(4)
            force_frame[0:3, 0:3] = rotation_matrix
            stiffness_vector = [k_x, K_MAX, K_MAX, K_ROT, K_ROT, K_ROT]

            print(f"Step {t}:")
            print(f"Executing Action: {step_action} \n Force Frame: {force_frame}\n Stiffness: {stiffness_vector}")
            input("press Enter to continue...")

            # æ¥ä¸‹æ¥éœ€è¦æŠŠæ•°æ®ï¼ˆå¤„ç†åï¼‰ä¼ ç»™agent
            agent.action(step_action,force_frame,stiffness_vector,rotation_rep = "rotation_6d")

            # time.sleep(0.1) åœ¨ action 
            # å¯èƒ½æœ‰ç‚¹é•¿äº†ï¼Œä¹Ÿå¯ä»¥æŠŠsleepæ”¾åœ¨è¿™é‡Œ
    
        agent.stop()


if __name__ == '__main__':
    reset_buffers()
    evaluate()
    # è€ƒè™‘æ”¹æˆä¼ å‚æ•°çš„è°ƒç”¨æ–¹æ³•